{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJT6KnQbwv5x"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import norm\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as stats\n",
        "import bspline\n",
        "import bspline.splinelab as splinelab\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzEoaP7Vw3fE"
      },
      "outputs": [],
      "source": [
        "data=pd.read_csv(<path to SovereignData.csv>)\n",
        "# [Year\t    Total Assets\tLog Return\tYearly Volatility\tQuarterly Volatility\tDistress Barrier\tRisk-free Rate\t    mu      Moody]\n",
        "#   0          1                2            3                      4                   5                       6            7        8\n",
        "\n",
        "# print(data.head)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df=data[['Year', 'Total Assets', 'Distress Barrier']]\n",
        "df.set_index('Year', inplace=True)\n",
        "\n",
        "# Create the line plot with markers\n",
        "plt.figure(figsize=(9, 4))  # Set the figure size\n",
        "\n",
        "lines = df.plot(marker='o', linestyle='-')  # Plot lines with markers\n",
        "\n",
        "# Add labels and title\n",
        "plt.ylabel('Value in USD')\n",
        "plt.title('Total Assets and Distress Barrier')\n",
        "plt.legend()\n",
        "\n",
        "# Customize plot (optional)\n",
        "# You can further customize the plot using matplotlib functions here\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVejKJ8kw3UR"
      },
      "outputs": [],
      "source": [
        "results=pd.DataFrame([], columns=['Year', 'Yearly Volatility', 'Quarterly Volatility', 'Quarter', 'N(d1)', 'd1', 'd2', 'N(-d2)', 'N(d1) Set2', 'd1 Set2', 'd2 Set2', 'N(-d2) Set2'])\n",
        "# we will have 5 values per year, per simulation, qtr num onwards all unique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5goxzo4bQVRI"
      },
      "outputs": [],
      "source": [
        "prices = pd.DataFrame([], columns=['Year', 'Total Assets', 'Distress Barrier', 'LCL 1', 'LCL 2', 'Mean LCL', 'Std LCL'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NormalizedProbability = pd.DataFrame([], columns=['Year', 'Moody Rating', 'N(-d2) Set 1', 'N(-d2) Set 2'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7Z3vP4Qw2_L"
      },
      "outputs": [],
      "source": [
        "T = 4\n",
        "M = 1         # maturity\n",
        "N_MC = 1000   # number of paths\n",
        "\n",
        "\n",
        "delta_t = M / T                # time interval\n",
        "\n",
        "\n",
        "# Define the risk aversion parameter\n",
        "risk_lambda = 0.001 # risk aversion parameter\n",
        "reg_param = 1e-3\n",
        "\n",
        "# disturbance level eta: Each action is multiplied by a uniform r.v. in the interval [1-eta, 1 + eta]\n",
        "eta = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5Q1AS3Ww27y"
      },
      "outputs": [],
      "source": [
        "# To get meaninful results, one should have ncolloc >= p+1\n",
        "p = 4 # order of spline (as-is; 3: cubic, 4: B-spline?)\n",
        "ncolloc = 12\n",
        "\n",
        "num_t_steps = T + 1\n",
        "num_basis =  ncolloc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0dtfMK8xA8x"
      },
      "outputs": [],
      "source": [
        "seed=42\n",
        "seed_1=84\n",
        "\n",
        "# standard normal random numbers\n",
        "np.random.seed(seed)  # You can use any integer as a seed\n",
        "RN = pd.DataFrame(np.random.randn(N_MC,T), index=range(1, N_MC+1), columns=range(1, T+1))\n",
        "\n",
        "np.random.seed(seed_1)  # You can use any integer as a seed\n",
        "RN_1 = pd.DataFrame(np.random.randn(N_MC,T), index=range(1, N_MC+1), columns=range(1, T+1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load RN from memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(RN.head)\n",
        "# print(RN_1.head)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jL3tV8EFxA5X"
      },
      "outputs": [],
      "source": [
        "def terminal_payoff(ST, K):\n",
        "    # ST   final stock price    # K    strike\n",
        "    payoff = max(ST-K, 0)\n",
        "    return payoff\n",
        "\n",
        "\n",
        "# functions to compute optimal hedges\n",
        "def function_A_vec(t,delta_S_hat,data_mat_t):\n",
        "    # Compute the matrix A_{nm} from Eq. (52) (with a regularization!)\n",
        "    X_mat = data_mat_t[t,:,:]\n",
        "    num_basis_funcs = X_mat.shape[1]\n",
        "    this_dS = delta_S_hat.loc[:,t].values\n",
        "    hat_dS2 = (this_dS**2).reshape(-1,1)\n",
        "    A_mat = np.dot(X_mat.T, X_mat * hat_dS2) + reg_param * np.eye(num_basis_funcs)\n",
        "    return A_mat\n",
        "\n",
        "\n",
        "def function_B_vec(t, Pi_hat, delta_S_hat, S, data_mat_t, gamma, mu, r):\n",
        "    # coef = 1.0/(2 * gamma * risk_lambda)\n",
        "    coef=0\n",
        "    tmp =  Pi_hat.loc[:,t+1] * delta_S_hat.loc[:,t] + coef * (np.exp(mu*delta_t) - np.exp(r*delta_t))* S.loc[:,t]\n",
        "    X_mat = data_mat_t[t,:,:]  # matrix of dimension N_MC x num_basis\n",
        "    B = np.dot(X_mat.T, tmp)\n",
        "    return B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-ChrLWo2Tsw"
      },
      "outputs": [],
      "source": [
        "def function_C_vec(t,data_mat_t,reg_param):\n",
        "    # Compute the matrix A_{nm} from Eq. (52) (with a regularization!)\n",
        "    X_mat = data_mat_t[t,:,:]\n",
        "    num_basis_funcs = X_mat.shape[1]\n",
        "    C_mat = np.dot(X_mat.T, X_mat) + reg_param * np.eye(num_basis_funcs)\n",
        "    return C_mat\n",
        "\n",
        "def function_D_vec(t, Q, R, data_mat_t, gamma):\n",
        "    X_mat = data_mat_t[t,:,:]\n",
        "    tmp = R.loc[:,t] + gamma * Q.loc[:,t+1]  # note that the second argument in Q is t+1 !\n",
        "    D = np.dot(X_mat.T, tmp.values)\n",
        "    return D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mFwJC0g2QaU"
      },
      "outputs": [],
      "source": [
        "# vectorized functions\n",
        "\n",
        "def function_S_vec(t,S_t_mat,reg_param):\n",
        "    # Compute the matrix S_{nm} from Eq. (75) (with a regularization!)\n",
        "    S_mat = S_t_mat[:,:,t]\n",
        "    S_mat_reg = S_mat + reg_param * np.eye(S_mat.shape[0])\n",
        "    return S_mat_reg\n",
        "\n",
        "# this function requires some refinement!\n",
        "def function_M_vec(t,\n",
        "                   Q_star, # max_Q_star,\n",
        "                   R,\n",
        "                   Psi_mat_t,  # 2D array of dimension 3M x num_MC\n",
        "                   gamma):\n",
        "\n",
        "    # Psi_mat = Psi_mat_t[:,:,t]   # 2D array of dimension 3M x num_MC\n",
        "    tmp = R.loc[:,t] + gamma * Q_star.iloc[:,t+1]\n",
        "    M_t = np.dot(Psi_mat_t, tmp.values)\n",
        "    return M_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwWVGab-2ZSh"
      },
      "outputs": [],
      "source": [
        "# The Black-Scholes prices\n",
        "def bs_put(t, S0, K, r, sigma):\n",
        "    d1 = (np.log(S0/K) + (r + 1/2 * sigma**2) * (T-t)) / sigma / np.sqrt(T-t)\n",
        "    d2 = (np.log(S0/K) + (r - 1/2 * sigma**2) * (T-t)) / sigma / np.sqrt(T-t)\n",
        "    price = K * np.exp(-r * (T-t)) * norm.cdf(-d2) - S0 * norm.cdf(-d1)\n",
        "    return price\n",
        "\n",
        "def bs_call(t, S0, K, r, sigma):\n",
        "    d1 = (np.log(S0/K) + (r + 1/2 * sigma**2) * (T-t)) / sigma / np.sqrt(T-t)\n",
        "    d2 = (np.log(S0/K) + (r - 1/2 * sigma**2) * (T-t)) / sigma / np.sqrt(T-t)\n",
        "    price = S0 * norm.cdf(d1) - K * np.exp(-r * (T-t)) * norm.cdf(d2)\n",
        "    return price"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "euJCKshwxA1Q",
        "outputId": "fb76de81-ffcf-4065-b7b0-5e5b04d6f852"
      },
      "outputs": [],
      "source": [
        "for i in range(1, 22): #use this to mean from 2002 to 2022\n",
        "    \n",
        "    ycnt=i\n",
        "    print(\"\\nSimulation\", i,\"/21: Year: \", i+2001 )\n",
        "\n",
        "    year=data.iloc[i, 0]            #year\n",
        "\n",
        "    S0=data.iloc[i, 1]                 #total asset value\n",
        "    S0=S0/1000000000\n",
        "\n",
        "    # log_return=data.iloc[i, 2]    #log return\n",
        "\n",
        "    sigma=data.iloc[i, 3]           #yearly volatility\n",
        "\n",
        "    sigma_qtr=data.iloc[i, 4]       #quartely volatility\n",
        "\n",
        "    K=data.iloc[i, 5]       #distress barrier\n",
        "    K=K/1000000000\n",
        "\n",
        "    r=data.iloc[i, 6]       #risk free rate\n",
        "    r=r/100\n",
        "\n",
        "    mu=data.iloc[i, 7]      # drift\n",
        "\n",
        "    moody_cat=data.iloc[i, 8]\n",
        "\n",
        "    gamma = np.exp(- r * delta_t)  # discount factor\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    S = pd.DataFrame([], index=range(1, N_MC+1), columns=range(T+1))\n",
        "    S.loc[:,0] = S0\n",
        "\n",
        "    S_1 = pd.DataFrame([], index=range(1, N_MC+1), columns=range(T+1))\n",
        "    S_1.loc[:,0] = S0\n",
        "\n",
        "    for t in range(1, T+1):\n",
        "      S.loc[:,t] = S.loc[:,t-1] * np.exp((mu - 1/2 * sigma**2) * delta_t + sigma * np.sqrt(delta_t) * RN.loc[:,t])\n",
        "      S_1.loc[:,t] = S_1.loc[:,t-1] * np.exp((mu - 1/2 * sigma**2) * delta_t + sigma * np.sqrt(delta_t) * RN_1.loc[:,t])\n",
        "\n",
        "    delta_S = S.loc[:,1:T].values - np.exp(r * delta_t) * S.loc[:,0:T-1]\n",
        "    delta_S_hat = delta_S.apply(lambda x: x - np.mean(x), axis=0)\n",
        "    X = - (mu - 1/2 * sigma**2) * np.arange(T+1) * delta_t + np.log(S.astype(float) / 1.0)  # delta_t here is due to their conventions\n",
        "\n",
        "    delta_S_1 = S_1.loc[:,1:T].values - np.exp(r * delta_t) * S_1.loc[:,0:T-1]\n",
        "    delta_S_hat_1 = delta_S_1.apply(lambda x: x - np.mean(x), axis=0)\n",
        "    X_1 = - (mu - 1/2 * sigma**2) * np.arange(T+1) * delta_t + np.log(S_1.astype(float) / 1.0)\n",
        "\n",
        "    # if ycnt>=20:\n",
        "    #   # plot 25 paths\n",
        "    #   step_size = N_MC // 25\n",
        "    #   idx_plot = np.arange(step_size, N_MC, step_size)\n",
        "    #   plt.plot(S.T.iloc[:, idx_plot])\n",
        "    #   plt.xlabel('Quarters')\n",
        "    #   plt.title('Asset Value Sample Paths')\n",
        "    #   plt.show()\n",
        "\n",
        "    #   plt.plot(X.T.iloc[:, idx_plot])\n",
        "    #   plt.xlabel('Quarters')\n",
        "    #   plt.ylabel('State Variable')\n",
        "    #   plt.title('State Variable Sample Paths')\n",
        "    #   plt.show()\n",
        "    \n",
        "\n",
        "    X_min = np.min(np.min(X))\n",
        "    X_max = np.max(np.max(X))\n",
        "    tau = np.linspace(X_min, X_max, ncolloc)  # These are the sites to which we would like to interpolate\n",
        "    k = splinelab.aptknt(tau, p)\n",
        "    # Spline basis of order p on knots k\n",
        "    basis = bspline.Bspline(k, p)\n",
        "    # f = plt.figure()\n",
        "    # basis.plot()\n",
        "\n",
        "\n",
        "    # Compute the data matrix\n",
        "    data_mat_t = np.zeros((num_t_steps, N_MC, num_basis))\n",
        "    data_mat_t_1 = np.zeros((num_t_steps, N_MC,num_basis ))\n",
        "\n",
        "    for i in np.arange(num_t_steps):\n",
        "      x = X.values[:,i]\n",
        "      basis_arr=np.array([ basis(i) for i in x ])\n",
        "      data_mat_t[i,:,:] = basis_arr\n",
        "\n",
        "      x_1 = X_1.values[:,i]\n",
        "      data_mat_t_1[i,:,:] = np.array([ basis(i) for i in x_1 ])\n",
        "\n",
        "\n",
        "    # Compute optimal hedge and portfolio value\n",
        "\n",
        "    # DP1\n",
        "    Pi = pd.DataFrame([], index=range(1, N_MC+1), columns=range(T+1))\n",
        "    Pi.iloc[:,-1] = S.iloc[:,-1].apply(lambda x: terminal_payoff(x, K))\n",
        "    Pi_hat = pd.DataFrame([], index=range(1, N_MC+1), columns=range(T+1))\n",
        "    Pi_hat.iloc[:,-1] = Pi.iloc[:,-1] - np.mean(Pi.iloc[:,-1])\n",
        "    a = pd.DataFrame([], index=range(1, N_MC+1), columns=range(T+1))\n",
        "    a.iloc[:,-1] = 0\n",
        "\n",
        "    # DP2\n",
        "    Pi_1 = pd.DataFrame([], index=range(1, N_MC+1), columns=range(T+1))\n",
        "    Pi_1.iloc[:,-1] = S_1.iloc[:,-1].apply(lambda x: terminal_payoff(x, K))\n",
        "    Pi_hat_1 = pd.DataFrame([], index=range(1, N_MC+1), columns=range(T+1))\n",
        "    Pi_hat_1.iloc[:,-1] = Pi_1.iloc[:,-1] - np.mean(Pi_1.iloc[:,-1])\n",
        "    a_1 = pd.DataFrame([], index=range(1, N_MC+1), columns=range(T+1))\n",
        "    a_1.iloc[:,-1] = 0\n",
        "\n",
        "    # Off Policy 1\n",
        "    a_op = pd.DataFrame([], index=range(1, N_MC+1), columns=range(T+1))\n",
        "    a_op.iloc[:,-1] = 0\n",
        "    Pi_op = pd.DataFrame([], index=range(1, N_MC+1), columns=range(T+1))\n",
        "    Pi_op.iloc[:,-1] = S.iloc[:,-1].apply(lambda x: terminal_payoff(x, K))\n",
        "    Pi_op_hat = pd.DataFrame([], index=range(1, N_MC+1), columns=range(T+1))\n",
        "    Pi_op_hat.iloc[:,-1] = Pi_op.iloc[:,-1] - np.mean(Pi_op.iloc[:,-1])\n",
        "    R_op = pd.DataFrame([], index=range(1, N_MC+1), columns=range(T+1))\n",
        "    R_op.iloc[:,-1] = - risk_lambda * np.var(Pi_op.iloc[:,-1])\n",
        "\n",
        "    # Off Policy 2\n",
        "    a_op_1 = pd.DataFrame([], index=range(1, N_MC+1), columns=range(T+1))\n",
        "    a_op_1.iloc[:,-1] = 0\n",
        "    Pi_op_1 = pd.DataFrame([], index=range(1, N_MC+1), columns=range(T+1))\n",
        "    Pi_op_1.iloc[:,-1] = S_1.iloc[:,-1].apply(lambda x: terminal_payoff(x, K))\n",
        "    Pi_op_hat_1 = pd.DataFrame([], index=range(1, N_MC+1), columns=range(T+1))\n",
        "    Pi_op_hat_1.iloc[:,-1] = Pi_op_1.iloc[:,-1] - np.mean(Pi_op_1.iloc[:,-1])\n",
        "    R_op_1 = pd.DataFrame([], index=range(1, N_MC+1), columns=range(T+1))\n",
        "    R_op_1.iloc[:,-1] = - risk_lambda * np.var(Pi_op_1.iloc[:,-1])\n",
        "\n",
        "\n",
        "    for t in range(T-1, -1, -1):\n",
        "      # DP1\n",
        "      A_mat = function_A_vec(t, delta_S_hat, data_mat_t)\n",
        "      B_vec = function_B_vec(t, Pi_hat, delta_S_hat, S, data_mat_t, gamma, mu, r)\n",
        "      A_mat = np.array(A_mat, dtype=float)\n",
        "      B_vec = np.array(B_vec, dtype=float)\n",
        "      phi = np.dot(np.linalg.inv(A_mat), B_vec)\n",
        "      a.loc[:,t] = np.dot(data_mat_t[t,:,:],phi)\n",
        "      Pi.loc[:,t] = gamma * (Pi.loc[:,t+1] - a.loc[:,t] * delta_S.loc[:,t])\n",
        "      Pi_hat.loc[:,t] = Pi.loc[:,t] - np.mean(Pi.loc[:,t])\n",
        "      # print(np.mean(a.iloc[:,t]))\n",
        "      \n",
        "\n",
        "      # DP2\n",
        "      A_mat_1 = function_A_vec(t,delta_S_hat_1,data_mat_t_1)\n",
        "      B_vec_1 = function_B_vec(t, Pi_hat_1, delta_S_hat_1, S_1, data_mat_t_1, gamma, mu, r)\n",
        "      A_mat_1 = np.array(A_mat_1, dtype=float)\n",
        "      B_vec_1 = np.array(B_vec_1, dtype=float)\n",
        "      phi_1 = np.dot(np.linalg.inv(A_mat_1), B_vec_1)\n",
        "      a_1.loc[:,t] = np.dot(data_mat_t_1[t,:,:],phi_1)\n",
        "      Pi_1.loc[:,t] = gamma * (Pi_1.loc[:,t+1] - a_1.loc[:,t] * delta_S_1.loc[:,t])\n",
        "      Pi_hat_1.loc[:,t] = Pi_1.loc[:,t] - np.mean(Pi_1.loc[:,t])\n",
        "\n",
        "\n",
        "      noise_factors = np.random.uniform(low=1-eta, high=1+eta, size=N_MC)\n",
        "\n",
        "      # Off Policy 1\n",
        "      a_op.loc[:,t] = np.dot(data_mat_t[t,:,:],phi)\n",
        "      a_op.loc[:, t] = noise_factors * a_op.loc[:, t]\n",
        "      Pi_op.loc[:,t] = gamma * (Pi_op.loc[:,t+1] - a_op.loc[:,t] * delta_S.loc[:,t])\n",
        "      Pi_op_hat.loc[:,t] = Pi_op.loc[:,t] - np.mean(Pi_op.loc[:,t])\n",
        "      R_op.loc[1:,t] = gamma * a_op.loc[1:,t] * delta_S.loc[1:,t] - risk_lambda * np.var(Pi_op.loc[1:,t])\n",
        "\n",
        "      # Off Policy 2\n",
        "      a_op_1.loc[:,t] = np.dot(data_mat_t_1[t,:,:],phi_1)\n",
        "      a_op_1.loc[:, t] = noise_factors * a_op_1.loc[:, t]\n",
        "      Pi_op_1.loc[:,t] = gamma * (Pi_op_1.loc[:,t+1] - a_op_1.loc[:,t] * delta_S_1.loc[:,t])\n",
        "      Pi_op_hat_1.loc[:,t] = Pi_op_1.loc[:,t] - np.mean(Pi_op_1.loc[:,t])\n",
        "      R_op_1.loc[1:,t] = gamma * a_op_1.loc[1:,t] * delta_S_1.loc[1:,t] - risk_lambda * np.var(Pi_op_1.loc[1:,t])\n",
        "\n",
        "    # print('inner loop done!')\n",
        "\n",
        "    # # plot 25 paths\n",
        "    # if ycnt>=20:\n",
        "    #   step_size = N_MC // 25\n",
        "    #   idx_plot = np.arange(step_size, N_MC, step_size)\n",
        "    #   plt.plot(a.T.iloc[:,idx_plot])\n",
        "    #   plt.xlabel('Quarters')\n",
        "    #   plt.title('Optimal Hedge')\n",
        "    #   plt.show()\n",
        "\n",
        "    #   plt.plot(Pi.T.iloc[:,idx_plot])\n",
        "    #   plt.xlabel('Quarters')\n",
        "    #   plt.title('Portfolio Value')\n",
        "    #   plt.show()\n",
        "\n",
        "\n",
        "    # save dp solution\n",
        "    a_dp = a.copy()\n",
        "    a_dp_1 = a_1.copy()\n",
        "\n",
        "    # overwrite dp variables (on-policy)vars with off policy variables\n",
        "    a = a_op.copy()\n",
        "    Pi = Pi_op.copy()\n",
        "    Pi_hat = Pi_op_hat.copy()\n",
        "    R = R_op.copy()\n",
        "\n",
        "    a_1 = a_op_1.copy()\n",
        "    Pi_1 = Pi_op_1.copy()\n",
        "    Pi_hat_1 = Pi_op_hat_1.copy()\n",
        "    R_1 = R_op_1.copy()\n",
        "\n",
        "    num_MC = a.shape[0]\n",
        "\n",
        "    ones_3d = np.ones((1,num_MC, T+1))\n",
        "    # make matrix A_t of shape (3 x num_MC x num_steps)\n",
        "    num_MC = a.shape[0]\n",
        "    a_1_1 = a.values.reshape((1,num_MC,T+1))\n",
        "    a_1_2 = 0.5 * a_1_1**2\n",
        "    A_stack = np.vstack((ones_3d, a_1_1, a_1_2))\n",
        "\n",
        "    # and the same for the second dataset:\n",
        "    a_2_1 = a_1.values.reshape((1,num_MC,T+1))\n",
        "    a_2_2 = 0.5 * a_2_1**2\n",
        "    A_stack_1 = np.vstack((ones_3d, a_2_1, a_2_2))\n",
        "\n",
        "    data_mat_swap_idx = np.swapaxes(data_mat_t,0,2)\n",
        "    data_mat_swap_idx_1 = np.swapaxes(data_mat_t_1,0,2)\n",
        "\n",
        "    # expand dimensions of matrices to multiply element-wise\n",
        "    A_2 = np.expand_dims(A_stack, axis=1) # becomes (3,1,1000,5)\n",
        "    data_mat_swap_idx = np.expand_dims(data_mat_swap_idx, axis=0)  # becomes (1,12,1000,5)\n",
        "    Psi_mat = np.multiply(A_2, data_mat_swap_idx) # this is a matrix of size 3 x num_basis x num_MC x num_steps\n",
        "\n",
        "    # now concatenate columns along the first dimension\n",
        "    Psi_mat = Psi_mat.reshape(-1, N_MC, T+1, order='F')\n",
        "\n",
        "    # the same for the twin dataset\n",
        "    A_2_1 = np.expand_dims(A_stack_1, axis=1)\n",
        "    data_mat_swap_idx_1 = np.expand_dims(data_mat_swap_idx_1, axis=0)\n",
        "    Psi_mat_1 = np.multiply(A_2_1, data_mat_swap_idx_1) # this is a matrix of size 3 x num_basis x num_MC x num_steps\n",
        "\n",
        "    # now concatenate columns along the first dimension\n",
        "    Psi_mat_1 = Psi_mat_1.reshape(-1, N_MC, T+1, order='F')\n",
        "\n",
        "    # make matrix S_t\n",
        "    Psi_1_aux = np.expand_dims(Psi_mat, axis=1)\n",
        "    Psi_2_aux = np.expand_dims(Psi_mat, axis=0)\n",
        "    S_t_mat = np.sum(np.multiply(Psi_1_aux, Psi_2_aux), axis=2)\n",
        "\n",
        "    Psi_1_aux_1 = np.expand_dims(Psi_mat_1, axis=1)\n",
        "    Psi_2_aux_1 = np.expand_dims(Psi_mat_1, axis=0)\n",
        "    S_t_mat_1 = np.sum(np.multiply(Psi_1_aux_1, Psi_2_aux_1), axis=2)\n",
        "\n",
        "    del Psi_1_aux, Psi_2_aux, Psi_1_aux_1, Psi_2_aux_1, data_mat_swap_idx,data_mat_swap_idx_1, A_2, A_2_1\n",
        "\n",
        "    # implied Q-function by input data (using the first form in Eq.(68))\n",
        "    Q_RL = pd.DataFrame([], index=range(1, N_MC+1), columns=range(T+1))\n",
        "    Q_RL.iloc[:,-1] = - Pi.iloc[:,-1] - risk_lambda * np.var(Pi.iloc[:,-1])\n",
        "\n",
        "    # similar variables for the twin dataset:\n",
        "    Q_RL_1 = pd.DataFrame([], index=range(1, N_MC+1), columns=range(T+1))\n",
        "    Q_RL_1.iloc[:,-1] = - Pi_1.iloc[:,-1] - risk_lambda * np.var(Pi_1.iloc[:,-1])\n",
        "\n",
        "    # optimal action\n",
        "    a_opt = np.zeros((N_MC,T+1))\n",
        "    a_star = pd.DataFrame([], index=range(1, N_MC+1), columns=range(T+1))\n",
        "    a_star.iloc[:,-1] = 0\n",
        "\n",
        "    # optimal action\n",
        "    a_opt_1 = np.zeros((N_MC,T+1))\n",
        "    a_star_1 = pd.DataFrame([], index=range(1, N_MC+1), columns=range(T+1))\n",
        "    a_star_1.iloc[:,-1] = 0\n",
        "\n",
        "    # optimal Q-function with optimal action\n",
        "    Q_star = pd.DataFrame([], index=range(1, N_MC+1), columns=range(T+1))\n",
        "    Q_star.iloc[:,-1] = Q_RL.iloc[:,-1]\n",
        "\n",
        "    # optimal Q-function with optimal action\n",
        "    Q_star_1 = pd.DataFrame([], index=range(1, N_MC+1), columns=range(T+1))\n",
        "    Q_star_1.iloc[:,-1] = Q_RL_1.iloc[:,-1]\n",
        "\n",
        "    max_Q_star = np.zeros((N_MC,T+1))\n",
        "    max_Q_star[:,-1] = Q_RL.iloc[:,-1].values\n",
        "\n",
        "    max_Q_star_1 = np.zeros((N_MC,T+1))\n",
        "    max_Q_star_1[:,-1] = Q_RL_1.iloc[:,-1].values\n",
        "\n",
        "    num_basis = data_mat_t.shape[2]\n",
        "\n",
        "    # The backward loop\n",
        "    for t in range(T-1, -1, -1):\n",
        "      # calculate vector W_t\n",
        "      S_mat_reg = function_S_vec(t,S_t_mat,reg_param)\n",
        "      M_t = function_M_vec(t,Q_star, R, Psi_mat[:,:,t], gamma)\n",
        "\n",
        "      S_mat_reg = np.array(S_mat_reg, dtype=float)\n",
        "      M_t = np.array(M_t, dtype=float)\n",
        "\n",
        "      W_t = np.dot(np.linalg.inv(S_mat_reg),M_t)  # this is an 1D array of dimension 3M\n",
        "\n",
        "      # reshape to a matrix W_mat\n",
        "      W_mat = W_t.reshape((3, num_basis), order='F')  # shape 3 x M\n",
        "\n",
        "      # make matrix Phi_mat\n",
        "      Phi_mat = data_mat_t[t,:,:].T  # dimension M x N_MC\n",
        "\n",
        "      # compute matrix U_mat of dimension N_MC x 3\n",
        "      U_mat = np.dot(W_mat, Phi_mat)\n",
        "\n",
        "      # twin variables for the twin dataset:\n",
        "      S_mat_reg_1 = function_S_vec(t,S_t_mat_1,reg_param)\n",
        "      M_t_1 = function_M_vec(t,Q_star_1, R_1, Psi_mat_1[:,:,t], gamma)\n",
        "      S_mat_reg_1 = np.array(S_mat_reg_1, dtype=float)\n",
        "      M_t_1 = np.array(M_t_1, dtype=float)\n",
        "      W_t_1 = np.dot(np.linalg.inv(S_mat_reg_1),M_t_1)  # this is an 1D array of dimension 3M\n",
        "\n",
        "      # reshape to a matrix W_mat\n",
        "      W_mat_1 = W_t_1.reshape((3, num_basis), order='F')  # shape 3 x M\n",
        "\n",
        "      # make matrix Phi_mat\n",
        "      Phi_mat_1 = data_mat_t_1[t,:,:].T  # dimension M x N_MC\n",
        "\n",
        "      # compute matrix U_mat_1 of dimension N_MC x 3\n",
        "      U_mat_1 = np.dot(W_mat_1, Phi_mat_1)\n",
        "\n",
        "      # compute vectors U_W^0,U_W^1,U_W^2 as rows of matrix U_mat\n",
        "      U_W_0 = U_mat[0,:]\n",
        "      U_W_1 = U_mat[1,:]\n",
        "      U_W_2 = U_mat[2,:]\n",
        "\n",
        "      U_W_0_1 = U_mat_1[0,:]\n",
        "      U_W_1_1 = U_mat_1[1,:]\n",
        "      U_W_2_1 = U_mat_1[2,:]\n",
        "\n",
        "      a_opt[:,t] = np.dot(data_mat_t[t,:,:], phi)\n",
        "      a_star.loc[:,t] = a_opt[:,t]\n",
        "\n",
        "      a_opt_1[:,t] = np.dot(data_mat_t_1[t,:,:], phi_1)\n",
        "      a_star_1.loc[:,t] = a_opt_1[:,t]\n",
        "\n",
        "      max_Q_star[:,t] = U_W_0 + a_opt[:,t] * U_W_1 + 0.5 * (a_opt[:,t]**2) * U_W_2\n",
        "      max_Q_star_1[:,t] = U_W_0_1 + a_opt_1[:,t] * U_W_1_1 + 0.5 * (a_opt_1[:,t]**2) * U_W_2_1\n",
        "\n",
        "      # update dataframes\n",
        "      Q_star.loc[:,t] = max_Q_star[:,t]\n",
        "\n",
        "      # update the Q_RL solution given by a dot product of two matrices W_t Psi_t\n",
        "      Psi_t = Psi_mat[:,:,t].T  # dimension N_MC x 3M\n",
        "      Q_RL.loc[:,t] = np.dot(Psi_t, W_t)\n",
        "\n",
        "      a_star_1.loc[:,t] = a_opt_1[:,t]\n",
        "      Q_star_1.loc[:,t] = max_Q_star_1[:,t]\n",
        "\n",
        "      # update the Q_RL solution given by a dot product of two matrices W_t Psi_t\n",
        "      Psi_t_1 = Psi_mat_1[:,:,t].T  # dimension N_MC x 3M\n",
        "      Q_RL_1.loc[:,t] = np.dot(Psi_t_1, W_t_1)\n",
        "\n",
        "      # trim outliers for Q_RL\n",
        "      up_percentile_Q_RL =  95 # 95\n",
        "      low_percentile_Q_RL = 5 # 5\n",
        "\n",
        "      low_perc_Q_RL, up_perc_Q_RL = np.percentile(Q_RL.loc[:,t],[low_percentile_Q_RL,up_percentile_Q_RL])\n",
        "\n",
        "      # trim outliers in values of max_Q_star:\n",
        "      flag_lower = Q_RL.loc[:,t].values < low_perc_Q_RL\n",
        "      flag_upper = Q_RL.loc[:,t].values > up_perc_Q_RL\n",
        "      Q_RL.loc[flag_lower,t] = low_perc_Q_RL\n",
        "      Q_RL.loc[flag_upper,t] = up_perc_Q_RL\n",
        "\n",
        "      low_perc_Q_RL_1, up_perc_Q_RL_1 = np.percentile(Q_RL_1.loc[:,t],[low_percentile_Q_RL,up_percentile_Q_RL])\n",
        "\n",
        "      # trim outliers in values of max_Q_star:\n",
        "      flag_lower_1 = Q_RL_1.loc[:,t].values < low_perc_Q_RL_1\n",
        "      flag_upper_1 = Q_RL_1.loc[:,t].values > up_perc_Q_RL_1\n",
        "      Q_RL_1.loc[flag_lower_1,t] = low_perc_Q_RL_1\n",
        "      Q_RL_1.loc[flag_upper_1,t] = up_perc_Q_RL_1\n",
        "\n",
        "    # print(\"fitted q loop done\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # dp1\n",
        "\n",
        "    Nd1_dp_1 = (a.mean()).astype(float)\n",
        "\n",
        "    Nd1_1=Nd1_dp_1[:4]\n",
        "    # print(\"Nd1_1\", Nd1_1)\n",
        "\n",
        "    d1_dp_1 = stats.zscore(Nd1_1)\n",
        "\n",
        "    d2_dp_1 = d1_dp_1.copy()\n",
        "    Nd2_dp_1 = d1_dp_1.copy()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # dp2\n",
        "\n",
        "    Nd1_dp_2 = (a_1.mean()).astype(float)\n",
        "\n",
        "    Nd1_2=Nd1_dp_2[:4]\n",
        "    # print(\"Nd1_2\", Nd1_2)\n",
        "\n",
        "    d1_dp_2 = stats.zscore(Nd1_2)\n",
        "\n",
        "    d2_dp_2 = d1_dp_1.copy()\n",
        "    Nd2_dp_2 = d1_dp_1.copy()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # loop for calculation\n",
        "    for qtr in range(0,T):\n",
        "\n",
        "      # dp1\n",
        "      d2_dp_1[qtr]=d1_dp_1[qtr] - sigma_qtr * np.sqrt((T-qtr)/T)\n",
        "      Nd2_dp_1[qtr]=norm.cdf(-d2_dp_1[qtr])\n",
        "\n",
        "\n",
        "      # dp2\n",
        "      d2_dp_2[qtr]=d1_dp_2[qtr]-sigma_qtr * np.sqrt((T-qtr)/T)\n",
        "      Nd2_dp_2[qtr]=norm.cdf(-d2_dp_2[qtr])\n",
        "\n",
        "    nd2_sum=0\n",
        "    nd2_sum_set2=0\n",
        "\n",
        "    # loop for data entry\n",
        "    for qtr in range(0,T):\n",
        "      # print to res matrix\n",
        "\n",
        "\n",
        "      nd2_sum+=Nd2_dp_1[qtr]\n",
        "      nd2_sum_set2+=Nd2_dp_2[qtr]\n",
        "\n",
        "      new_res_row = {\n",
        "        'Year': year,\n",
        "        'Yearly Volatility': sigma,\n",
        "        'Quarterly Volatility': sigma_qtr,\n",
        "        'Quarter': qtr,\n",
        "        'N(d1)': Nd1_dp_1[qtr],\n",
        "        'd1': d1_dp_1[qtr],\n",
        "        'd2': d2_dp_1[qtr],\n",
        "        'N(-d2)': Nd2_dp_1[qtr],\n",
        "        'N(d1) Set2': Nd1_dp_2[qtr],\n",
        "        'd1 Set2': d1_dp_2[qtr],\n",
        "        'd2 Set2': d2_dp_2[qtr],\n",
        "        'N(-d2) Set2': Nd2_dp_2[qtr]\n",
        "      }\n",
        "      new_row_df = pd.DataFrame([new_res_row])\n",
        "      results = pd.concat([results, new_row_df], ignore_index=False)\n",
        "\n",
        "    new_norm_row={\n",
        "      'Year': year,\n",
        "      'Moody Rating': moody_cat,\n",
        "      'N(-d2) Set 1': nd2_sum/5,\n",
        "      'N(-d2) Set 2': nd2_sum_set2/5,\n",
        "    }\n",
        "    new_row_df = pd.DataFrame([new_norm_row])\n",
        "    NormalizedProbability = pd.concat([NormalizedProbability, new_row_df], ignore_index=False)\n",
        "\n",
        "\n",
        "    # QLBS option price\n",
        "    C_QLBS = - Q_star.copy()\n",
        "    qlbs_price_1 = np.mean(C_QLBS.iloc[:,0])\n",
        "\n",
        "    C_QLBS_1 = - Q_star_1.copy()\n",
        "    qlbs_price_2 = np.mean(C_QLBS_1.iloc[:,0])\n",
        "\n",
        "    QLBS_prices = np.array([C_QLBS.iloc[0,0],C_QLBS_1.iloc[0,0]])\n",
        "    qlbs_mean_price = np.mean(QLBS_prices)\n",
        "    qlbs_std_price = np.std(QLBS_prices)\n",
        "\n",
        "    new_price_row = {\n",
        "      'Year': year,\n",
        "      'Total Assets': S0,\n",
        "      'Distress Barrier': K,\n",
        "      'LCL 1': qlbs_price_1,\n",
        "      'LCL 2': qlbs_price_2,\n",
        "      'Mean LCL': qlbs_mean_price,\n",
        "      'Std LCL': qlbs_std_price\n",
        "    }\n",
        "    new_row_df = pd.DataFrame([new_price_row])\n",
        "    prices = pd.concat([prices, new_row_df], ignore_index=False)\n",
        "\n",
        "\n",
        "\n",
        "    print('---------------------------------')\n",
        "    print('       QLBS RL Option Pricing       ')\n",
        "    print('---------------------------------\\n')\n",
        "    print('%-25s' % ('Initial Stock Price:'), S0)\n",
        "    # print('%-25s' % ('Drift of Stock:'), mu)\n",
        "    print('%-25s' % ('Volatility of Stock:'), sigma)\n",
        "    print('%-25s' % ('Risk-free Rate:'), r)\n",
        "    print('%-25s' % ('Risk aversion parameter :'), risk_lambda)\n",
        "    print('%-25s' % ('Strike:'), K)\n",
        "    # print('%-25s' % ('Maturity:'), M)\n",
        "    print('%-26s %.4f' % ('\\nThe QLBS Call Price 1 :', (np.mean(C_QLBS.iloc[:,0]))))\n",
        "    print('%-26s %.4f' % ('The QLBS Call Price 2 :', (np.mean(C_QLBS_1.iloc[:,0]))))\n",
        "\n",
        "\n",
        "    QLBS_prices = np.array([C_QLBS.iloc[0,0],C_QLBS_1.iloc[0,0]])\n",
        "    mean_price = np.mean(QLBS_prices)\n",
        "    std_price = np.std(QLBS_prices)\n",
        "\n",
        "    print('%-26s  %.4f +/- %.4f ' % ('\\nQLBS Call Price: ',mean_price,std_price ))\n",
        "    print('%-26s %.4f' % ('\\nBlack-Scholes Call Price:', bs_call(0, S0, K, r, sigma)))\n",
        "    print('\\n')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(NormalizedProbability)\n",
        "\n",
        "#currently has three columns, year, nd2 set 1, nd2 set 2                                +3\n",
        "\n",
        "#step 0: should also have moody's own ratings                                           +1\n",
        "\n",
        "#step 1: add min max normalized values for these themselves                             +2\n",
        "\n",
        "#step 2: come up with a scheme to generate categorical moody ratings for each set       +2\n",
        "\n",
        "#step 3: convert the categories to numerical values for moody, set 1, set2              +3\n",
        "\n",
        "# Step 4: euclidean distance between  \n",
        "            # norm moody, norm moody set1 \n",
        "            # norm moody, norm moody set 2\n",
        "            # norm set 1, norm moody set 1\n",
        "            # norm set 2, norm moody set 2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler=MinMaxScaler()\n",
        "NormalizedProbability['Normalized N(-d2) Set 1'] = scaler.fit_transform(NormalizedProbability[['N(-d2) Set 1']])\n",
        "NormalizedProbability['Normalized N(-d2) Set 2'] = scaler.fit_transform(NormalizedProbability[['N(-d2) Set 2']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def default_risk_rating(probability):\n",
        "    ratings = {\n",
        "        'B1': (0.0, 0.25),  # highest rating\n",
        "        'B2': (0.25, 0.5),\n",
        "        'B3': (0.5, 0.75),\n",
        "        'Caa1': (0.75, 1.0)  # lowest rating\n",
        "    }\n",
        "    for rating, (lower, upper) in ratings.items():\n",
        "        if lower <= probability <= upper:\n",
        "            return rating\n",
        "    return 'Unknown'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NormalizedProbability['Moody Set 1'] = NormalizedProbability['Normalized N(-d2) Set 1'].apply(default_risk_rating)\n",
        "NormalizedProbability['Moody Set 2'] = NormalizedProbability['Normalized N(-d2) Set 2'].apply(default_risk_rating)\n",
        "\n",
        "# NormalizedProbability['Moody Set 1'] = NormalizedProbability['N(-d2) Set 1'].apply(default_risk_rating)\n",
        "# NormalizedProbability['Moody Set 2'] = NormalizedProbability['N(-d2) Set 2'].apply(default_risk_rating)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "\n",
        "# Create a LabelEncoder object\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Create a MinMaxScaler object\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Define the ratings in order from best to worst\n",
        "ratings = ['B1', 'B2', 'B3', 'Caa1']\n",
        "\n",
        "# Encode and normalize each column\n",
        "for col in ['Moody Rating', 'Moody Set 1', 'Moody Set 2']:\n",
        "\n",
        "    # Encode the ratings\n",
        "    NormalizedProbability[f'{col}_encoded'] = le.fit_transform(NormalizedProbability[col])\n",
        "\n",
        "    # Normalize the encoded values\n",
        "    NormalizedProbability[f'{col}_normalized'] = scaler.fit_transform(NormalizedProbability[[f'{col}_encoded']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Calculate Euclidean distances\n",
        "NormalizedProbability['Euc_Set1-vs-Moody'] = NormalizedProbability.apply(lambda row: np.linalg.norm([row['Moody Rating_normalized'], row['Moody Set 1_normalized']]), axis=1)\n",
        "NormalizedProbability['Euc_Set1-vs-Moody'] = NormalizedProbability.apply(lambda row: np.linalg.norm([row['Moody Rating_normalized'], row['Moody Set 2_normalized']]), axis=1)\n",
        "NormalizedProbability['Euc_Set1-vs-NormSet1'] = NormalizedProbability.apply(lambda row: np.linalg.norm([row['Normalized N(-d2) Set 1'], row['Moody Set 1_normalized']]), axis=1)\n",
        "NormalizedProbability['Euc_Set2-vs-NormSet2'] = NormalizedProbability.apply(lambda row: np.linalg.norm([row['Normalized N(-d2) Set 2'], row['Moody Set 2_normalized']]), axis=1)\n",
        "\n",
        "NormalizedProbability['Euc_Set1-vs-Nd2_1'] = NormalizedProbability.apply(lambda row: np.linalg.norm([row['N(-d2) Set 1'], row['Moody Set 1_normalized']]), axis=1)\n",
        "NormalizedProbability['Euc_Set2-vs-Nd2_2'] = NormalizedProbability.apply(lambda row: np.linalg.norm([row['N(-d2) Set 2'], row['Moody Set 2_normalized']]), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "q1zyho5PAt8c",
        "outputId": "c6ed39e3-fcd1-484a-c3c2-e08f55e7a2d5"
      },
      "outputs": [],
      "source": [
        "print(results.head(8))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6v193rtWZvCQ",
        "outputId": "e580a0d3-b123-4f58-d697-6f3b118a2dd2"
      },
      "outputs": [],
      "source": [
        "print(prices.head(8))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(NormalizedProbability)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7-B__eaaFtxp"
      },
      "outputs": [],
      "source": [
        "NormalizedProbability.to_csv('Probabilities.csv')\n",
        "\n",
        "results.to_csv('Variables.csv')\n",
        "\n",
        "prices.to_csv('LCL.csv')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
